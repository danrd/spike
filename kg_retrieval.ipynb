{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -U sentence-transformers\n",
    "%pip install hnswlib\n",
    "%pip install SPARQLWrapper\n",
    "%pip install rdflib\n",
    "%pip install spacy\n",
    "# python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import hnswlib\n",
    "import pickle\n",
    "import rdflib\n",
    "import SPARQLWrapper\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from io import StringIO\n",
    "from rdflib import Graph\n",
    "from SPARQLWrapper import SPARQLWrapper, JSON, RDF, XML \n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L12-v2')\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "stopwords = nlp.Defaults.stop_words\n",
    "\n",
    "data_path = os.getcwd()+'\\\\data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(data_path, 'id2pred.pickle'), \"rb\") as input_file:\n",
    "    id2pred = pickle.load(input_file) \n",
    "    \n",
    "with open(os.path.join(data_path, 'pred2id.pickle'), \"rb\") as input_file:\n",
    "    pred2id = pickle.load(input_file)    \n",
    "    \n",
    "with open(os.path.join(data_path, 'pred2emb.pickle'), \"rb\") as input_file:\n",
    "    pred2emb = pickle.load(input_file)  \n",
    "\n",
    "with open(os.path.join(data_path, 'id2res.pickle'), \"rb\") as input_file:\n",
    "    id2res = pickle.load(input_file)\n",
    "    \n",
    "# with open(os.path.join(data_path, 'res2id.pickle'), \"rb\") as input_file:\n",
    "#     res2id = pickle.load(input_file)    \n",
    "\n",
    "# with open(os.path.join(data_path, 'res2emb.pickle'), \"rb\") as input_file:   \n",
    "#     res2emb = pickle.load(input_file)\n",
    "\n",
    "with open(os.path.join(data_path, 'contrib2emb.pickle'), \"rb\") as input_file: \n",
    "    contrib2emb = pickle.load(input_file) \n",
    "\n",
    "with open(os.path.join(data_path, 'contrib2id.pickle'), \"rb\") as input_file: \n",
    "    contrib2id = pickle.load(input_file)\n",
    "\n",
    "with open(os.path.join(data_path, 'contrib2emb.pickle'), \"rb\") as input_file:    \n",
    "    contrib2emb = pickle.load(input_file)    \n",
    "    \n",
    "      \n",
    "with open(os.path.join(data_path, 'id2paper.pickle'), \"rb\") as input_file:     \n",
    "    id2paper = pickle.load(input_file)\n",
    "    \n",
    "with open(os.path.join(data_path, 'paper2id.pickle'), \"rb\") as input_file: \n",
    "    paper2id = pickle.load(input_file)    \n",
    "    \n",
    "with open(os.path.join(data_path, 'paper2emb.pickle'), \"rb\") as input_file:    \n",
    "    paper2emb = pickle.load(input_file)\n",
    "    \n",
    "\n",
    "with open(os.path.join(data_path, 'pred_index.pickle'), \"rb\") as input_file:     \n",
    "    pred_index = pickle.load(input_file) \n",
    "\n",
    "# with open(os.path.join(data_path, 'res_index.pickle'), \"rb\") as input_file:    \n",
    "#     res_index = pickle.load(input_file)  \n",
    "\n",
    "with open(os.path.join(data_path, 'contrib_index.pickle'), \"rb\") as input_file:    \n",
    "    contrib_index = pickle.load(input_file)     \n",
    "\n",
    "with open(os.path.join(data_path, 'paper_index.pickle'), \"rb\") as input_file: \n",
    "    paper_index = pickle.load(input_file)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_q(question:str):\n",
    "    \"\"\"\n",
    "    filter out unnecessary words from a question\n",
    "    \"\"\"\n",
    "    res = []\n",
    "    tok_pos = []\n",
    "    for token in nlp(question):\n",
    "        tok_pos.append((token.lemma_, token.pos_))\n",
    "        if token.pos_ in [\"NOUN\", \"PROPN\", \"ADJ\", \"VERB\"]: # keep only nouns, adjectives and verbs\n",
    "            if token.lemma_ not in stopwords:\n",
    "                res.append(str(token))\n",
    "    return \" \".join(res)\n",
    "\n",
    "def create_n_grams(sentence, n=2):\n",
    "    \"\"\"\n",
    "    create n_grams for a sentence after its filtering\n",
    "    \"\"\"\n",
    "    filtered_sentence = filter_q(sentence)\n",
    "    words = filtered_sentence.split(\" \")\n",
    "    n_grams = []\n",
    "    text_l = len(words)\n",
    "    for i in range(2, n+1):\n",
    "        for j in range(text_l-i):\n",
    "            n_gram = words[j:j+i]\n",
    "            n_gram_text = \" \".join(n_gram)\n",
    "            n_grams.append(n_gram_text)\n",
    "    return n_grams\n",
    "\n",
    "def set_prefixes(graph, prefixes=[]):\n",
    "    \"\"\"\n",
    "    set all prefixes from a list for a graph\n",
    "    \"\"\"\n",
    "    for prefix in prefixes:\n",
    "        graph.bind(prefix[0], prefix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def n_closest(question, source, n, n_gram):\n",
    "    \"\"\"\n",
    "    return n elements of orkg relevant for a question\n",
    "    \"\"\"\n",
    "    if n_gram:\n",
    "        filteted_text = filter_q(question)\n",
    "        texts = create_n_grams(filteted_text)\n",
    "    else:\n",
    "        texts = [filter_q(question)]\n",
    "    if source==\"pred\":\n",
    "        tuples = []\n",
    "        for text in texts:\n",
    "            out, scores = pred_index.knn_query(model.encode(text, show_progress_bar=False), k=n)\n",
    "            tuples.extend(list(zip(out[0], scores[0])))\n",
    "        sorted_out = sort_tuples(tuples)[0:n]\n",
    "        all_preds = list(pred2emb.keys())\n",
    "        output = [all_preds[pair[0]] for pair in sorted_out]\n",
    "        return output\n",
    "    elif source==\"res\":\n",
    "        tuples = []\n",
    "        for text in texts:\n",
    "            out, scores = res_index.knn_query(model.encode(text, show_progress_bar=False), k=n)\n",
    "            tuples.extend(list(zip(out[0], scores[0])))\n",
    "        sorted_out = sort_tuples(tuples)[0:n]\n",
    "        all_res = list(res2emb.keys())\n",
    "        output = [all_res[pair[0]] for pair in sorted_out]\n",
    "        return output\n",
    "    elif source==\"paper\":\n",
    "        tuples = []\n",
    "        for text in texts:\n",
    "            out, scores = paper_index.knn_query(model.encode(text, show_progress_bar=False), k=n)\n",
    "            tuples.extend(list(zip(out[0], scores[0])))\n",
    "        sorted_out = sort_tuples(tuples)[0:n]\n",
    "        all_papers = list(paper2emb.keys())\n",
    "        papers = [all_papers[pair[0]] for pair in sorted_out]\n",
    "        output = [str(uri) for uri in papers]\n",
    "        return output\n",
    "    elif source==\"contrib\":\n",
    "        tuples = []\n",
    "        for text in texts:\n",
    "            out, scores = contrib_index.knn_query(model.encode(text, show_progress_bar=False), k=n)\n",
    "            tuples.extend(list(zip(out[0], scores[0])))\n",
    "        sorted_out = sort_tuples(tuples)[0:n]\n",
    "        all_contribs = list(contrib2emb.keys())\n",
    "        contribs = [all_contribs[pair[0]] for pair in sorted_out]\n",
    "        output = [str(uri) for uri in contribs]\n",
    "        return output\n",
    "    \n",
    "def sort_tuples(l):\n",
    "    out = []\n",
    "    black_list = []\n",
    "    tuples = sorted(l, key=lambda x: x[1], reverse=False)\n",
    "    for tup in tuples:\n",
    "        if tup[0] in black_list:\n",
    "            continue\n",
    "        else:\n",
    "            out.append(tup)\n",
    "            black_list.append(tup[0])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_subgraph(triples=[], predicates_list=[]):\n",
    "    \"\"\"\n",
    "    return a graph for a list of triples\n",
    "    \"\"\"\n",
    "    res_graph = Graph()\n",
    "    set_prefixes(res_graph, prefixes)\n",
    "    for ind, triple in enumerate(triples):\n",
    "        pred_id = str(triple[1]).split(\"/\")[-1]\n",
    "        try:\n",
    "            pred = id2pred[pred_id]\n",
    "        except Exception as e:\n",
    "            continue\n",
    "        if pred not in predicates_list:\n",
    "            continue\n",
    "        else:\n",
    "            text_triple = swap_prefixes(triple)\n",
    "            res_graph.add(text_triple) \n",
    "    return res_graph\n",
    "\n",
    "def get_subgraph_string(graph):\n",
    "    \"\"\"\n",
    "    return string representation of a graph\n",
    "    \"\"\"\n",
    "    tmp = sys.stdout\n",
    "    my_result = StringIO()\n",
    "    sys.stdout = my_result\n",
    "    graph.print() \n",
    "    sys.stdout = tmp\n",
    "    return my_result.getvalue()\n",
    "\n",
    "def swap_prefixes(triple):\n",
    "    \"\"\"\n",
    "    replace ids in a triple with textual representation\n",
    "    \"\"\"\n",
    "    subj_id = str(triple[0]).split(\"/\")[-1]\n",
    "    subj_base = str(triple[0]).split(\"/\")[0:-1]\n",
    "    if subj_id in id2res.keys():\n",
    "        subj = id2res[subj_id].replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "        subj_base.append(subj)\n",
    "        triple[0] = rdflib.term.URIRef(\"/\".join(subj_base))\n",
    "    \n",
    "\n",
    "        pred_id = str(triple[1]).split(\"/\")[-1]\n",
    "        pred_base = str(triple[1]).split(\"/\")[0:-1]\n",
    "    if pred_id in id2pred.keys():\n",
    "        pred = id2pred[pred_id].replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "        pred_base.append(pred)\n",
    "        triple[1] = rdflib.term.URIRef(\"/\".join(pred_base))\n",
    "    \n",
    "    obj_id = str(triple[2]).split(\"/\")[-1]\n",
    "    obj_base = str(triple[2]).split(\"/\")[0:-1]\n",
    "    if obj_id in id2res.keys():\n",
    "        obj = id2res[obj_id].replace(\" \", \"_\").replace(\":\", \"_\")\n",
    "        obj_base.append(obj)\n",
    "        triple[2] = rdflib.term.URIRef(\"/\".join(obj_base))\n",
    "    return triple    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(paper_title, predicates_list, graph):\n",
    "    \"\"\"\n",
    "    return a subgraph of orkg for a paper using only relevant predicates\n",
    "    \"\"\"\n",
    "    paper_id = paper2id[paper_title]\n",
    "    left = '{'\n",
    "    right = '}'\n",
    "    middle = f'orkgr:{paper_id} ?x ?y. ?y ?pred ?label'\n",
    "    template = f'PREFIX orkgp: <http://orkg.org/orkg/predicate/> PREFIX orkgc: <http://orkg.org/orkg/class/> PREFIX orkgr: <http://orkg.org/orkg/resource/> SELECT ?pred ?label WHERE {left}{middle}{right}'\n",
    "    result = graph.query(template)\n",
    "    triples = []\n",
    "    for triple in result:\n",
    "        triples.append([rdflib.term.URIRef(f'http://orkg.org/orkg/resource/{paper_id }'), triple[0], triple[1]])\n",
    "    graph = construct_subgraph(triples, predicates_list)\n",
    "    return graph\n",
    "\n",
    "def process_contrib(contrib_title, predicates_list, graph):\n",
    "    \"\"\"\n",
    "    return a subgraph of orkg for a contribution using only relevant predicates\n",
    "    \"\"\"\n",
    "    contrib_id = contrib2id[contrib_title]\n",
    "    left = '{'\n",
    "    right = '}'\n",
    "    middle = f'orkgr:{contrib_id} ?x ?y'\n",
    "    template = f'PREFIX orkgp: <http://orkg.org/orkg/predicate/> PREFIX orkgc: <http://orkg.org/orkg/class/> PREFIX orkgr: <http://orkg.org/orkg/resource/> SELECT ?x ?y WHERE {left}{middle}{right}'\n",
    "    result = graph.query(template)\n",
    "    triples = []\n",
    "    for triple in result:\n",
    "        triples.append([rdflib.term.URIRef(f'http://orkg.org/orkg/resource/{contrib_id }'), triple[0], triple[1]])\n",
    "    graph = construct_subgraph(triples, predicates_list)\n",
    "    return graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orkg = Graph()\n",
    "orkg.parse(os.path.join(data_path, 'rdf-export-orkg.nt'))\n",
    "prefixes = [\n",
    "['orkgp', 'http://orkg.org/orkg/predicate/'],\n",
    "['orkgc', 'http://orkg.org/orkg/class/'],\n",
    "['orkgr', 'http://orkg.org/orkg/resource/'],\n",
    "['rdfs', 'http://www.w3.org/2000/01/rdf-schema#>'],\n",
    "['rdf', 'http://www.w3.org/1999/02/22-rdf-syntax-ns#'],\n",
    "['xsd', 'http://www.w3.org/2001/XMLSchema#']   \n",
    "]\n",
    "set_prefixes(orkg, prefixes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_subgraph(question:str, main_graph:rdflib.Graph, n_preds:int=1000, n_papers:int=10, n_contribs:int=10):\n",
    "    \"\"\"\n",
    "    return a number of orkg subrgraphs related to a question\n",
    "    \"\"\"\n",
    "    subgraphs = []\n",
    "    predicates_list = n_closest(question, 'pred', n_preds, True)\n",
    "    papers_list = n_closest(question, 'paper', n_papers, True)\n",
    "    contribs_list = n_closest(question, 'contrib', n_contribs, True)\n",
    "    for paper in papers_list:\n",
    "        graph = process_paper(paper, predicates_list, main_graph)\n",
    "        if len(graph) > 0:\n",
    "            graph_text = get_subgraph_string(graph)\n",
    "            subgraphs.append(graph_text) \n",
    "    for contrib in contribs_list:\n",
    "        graph = process_contrib(contrib, predicates_list, main_graph)\n",
    "        if len(graph) > 0:\n",
    "            graph_text = get_subgraph_string(graph)\n",
    "            subgraphs.append(graph_text) \n",
    "    return subgraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "q1 = \"Which model has achieved the highest Accuracy score on the Story Cloze Test benchmark dataset?\"\n",
    "q2 = \"List the title and ID of research papers that contain a benchmark over the Penn Treebank (Word Level) dataset?\"\n",
    "q3 = \"What models are being evaluated on the UrbanSound8k dataset?\"\n",
    "q4 = \"Provide a list of research paper titles and IDs that have benchmarked models on the Penn Treebank dataset?\"\n",
    "q5 = \"What models are being evaluated on the TDMSci dataset?\"\n",
    "q6 = \"What is the mean capacity of a carbon-based fuel?\"\n",
    "q7 = \"Give me a list of research papers along with their titles and IDs, that have performed benchmarks on the MLDoc Zero-Shot English-to-Russian dataset?\"\n",
    "q8 = \"Indicate the model that performed best in terms of Accuracy metric on the Kuzushiji-MNIST benchmark dataset?\"\n",
    "q9 = \"Which model has achieved the highest BLEU score score on the WMT2016 Romanian-English benchmark dataset?\"\n",
    "q10 = \"What is the highest benchmark result achieved on the Ball in cup, catch (DMControl500k) dataset, including the metric and its value?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n",
      "http://www.w3.org/2000/01/rdf-schema#> does not look like a valid URI, trying to serialize this will break.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n\\norkgr:A_Simple_and_Effective_Approach_to_the_Story_Cloze_Test orkgp:Benchmark orkgr:Benchmark_Story_Cloze_Test ;\\n    orkgp:model orkgr:Val-ls-skip .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:A_benchmarking_method_for_information_systems orkgp:description \"Covers design tools, software metrics, testing and debugging, programming environments, etc\"^^xsd:string .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n\\norkgr:Towards_Better_Accuracy-efficiency_Trade-offs__Divide_and_Co-training orkgp:Benchmark orkgr:Benchmark_CIFAR-10,\\n        orkgr:Benchmark_CIFAR-100,\\n        orkgr:Benchmark_ImageNet ;\\n    orkgp:model orkgr:Densenet-bc-190_s_4,\\n        orkgr:Pyramidnet-272_s_4,\\n        orkgr:Resnext-101_64x4d_s_2_224px,\\n        orkgr:Se-resnext-101_64x4d_s_2_416px,\\n        orkgr:Shake-shake_26_2x96d_s_4,\\n        orkgr:Wrn-28-10_s_4,\\n        orkgr:Wrn-40-10_s_4 .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n\\norkgr:Soft_Truncation__A_Universal_Training_Technique_of_Score-based_Diffusion_Model_for_High_Precision_Score_Estimation orkgp:Benchmark orkgr:Benchmark_CIFAR-10,\\n        orkgr:Benchmark_CelebA-HQ_256x256,\\n        orkgr:Benchmark_CelebA_64x64,\\n        orkgr:Benchmark_FFHQ_256_x_256,\\n        orkgr:Benchmark_ImageNet_32x32,\\n        orkgr:Benchmark_LSUN_Bedroom_256_x_256,\\n        orkgr:Benchmark_STL-10 ;\\n    orkgp:model orkgr:Ddpm_vp_fid_st,\\n        orkgr:Ddpm_vp_nll_st,\\n        orkgr:Udm_rve_st,\\n        orkgr:Uncsn_rve_st .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n\\norkgr:Collaboration_of_Experts__Achieving_80%_Top-1_Accuracy_on_ImageNet_with_100M_FLOPs orkgp:Benchmark orkgr:Benchmark_ImageNet ;\\n    orkgp:model orkgr:Coe-large_194_mflops,\\n        orkgr:Coe-large_214_mflops,\\n        orkgr:Coe-small_100_mflops .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:International_Phyical_Performance_Test_Profile_\\\\(IPPTP\\\\) orkgp:Standard_deviation 0.363988071680069 ;\\n    orkgp:test_year 1988 .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:_International_Phyical_Performance_Test_Profile_\\\\(IPPTP\\\\) orkgp:Standard_deviation 0.300661206245422 ;\\n    orkgp:test_year 1988 .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n\\norkgr:Automatic_Diagnosis_of_Attention_Deficit_Hyperactivity_Disorder_Using_Machine_Learning orkgp:contribution orkgr:Decision_tree,\\n        orkgr:K-nearest_Neighbour,\\n        orkgr:Naive_Bayes,\\n        orkgr:Random_forest,\\n        orkgr:Support_vector_machine,\\n        orkgr:logistic_regression .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:Record_Identifier__14311001 orkgp:Standard_deviation 0.363988071680069 ;\\n    orkgp:test_year 1988 .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:Record_Identifier__14311201 orkgp:Standard_deviation 0.300661206245422 ;\\n    orkgp:test_year 1988 .\\n\\n\\n',\n",
       " '@prefix orkgp: <http://orkg.org/orkg/predicate/> .\\n@prefix orkgr: <http://orkg.org/orkg/resource/> .\\n@prefix xsd: <http://www.w3.org/2001/XMLSchema#> .\\n\\norkgr:Contribution_1 orkgp:Algorithm\\\\(s\\\\) orkgr:NSGA-II_NSGA-II ;\\n    orkgp:Number_of_Objectives \"2\"^^xsd:string ;\\n    orkgp:Quality_Indicators <http://orkg.org/orkg/resource/--> .\\n\\n\\n']"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieve_subgraph(q1, orkg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spike",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
